{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab  3 - Pytorch\n",
    "\n",
    "##### Keywords: gradient descent, logistic regression, pytorch, sgd, minibatch sgd\n",
    "##### Data: data/iris_dataset.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "{:.no_toc}\n",
    "* \n",
    "{: toc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Aims\n",
    "\n",
    "- Introduction to PyTorch\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Automatic differentiation\n",
    "- Gradient descent\n",
    "\n",
    "## Lab Trajectory\n",
    "\n",
    "- PyTorch Installation\n",
    "- Why PyTorch?\n",
    "- Working with PyTorch Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing PyTorch\n",
    "\n",
    "### Installation\n",
    "\n",
    "#### OS X/Linux \n",
    "We shall be using PyTorch in this class.  Please go to the PyTorch website where they have a nicely designed interface for installation instructions depending on your OS (Linux/OS X), your package management system (pip/conda) and your CUDA install (8/9/none).  http://pytorch.org.  Your installation instructions will look something like:\n",
    "\n",
    "- conda install pytorch torchvision -c pytorch \n",
    "\n",
    "or\n",
    "\n",
    "- pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
    "- pip3 install torchvision\n",
    "\n",
    "#### Windows\n",
    "PyTorch doesn't have official Windows support as yet, but there are Windows binaries available due to Github user @peter123.  Please see his PyTorch for Windows repo https://github.com/peterjc123/pytorch-scripts for installation instructions for different versions of Windows and CUDA.  In all likelihood your installation instructions will be:\n",
    "\n",
    "- conda install -c peterjc123 pytorch\n",
    "- pip install torchvision\n",
    "\n",
    "\n",
    "#### Testing Installation\n",
    "\n",
    "If the code cell shows an error, then your PyTorch installation is not working and you should contact one of the teaching staff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.0.post4\n",
      "0.2.0\n",
      "\n",
      " 0.2901  0.8863  0.4383\n",
      " 0.9738  0.9825  0.1046\n",
      " 0.8069  0.1135  0.3565\n",
      " 0.4906  0.4698  0.9623\n",
      " 0.1116  0.4729  0.3536\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "-8.447796634522254\n"
     ]
    }
   ],
   "source": [
    "### Code Cell to Test PyTorch\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "print(torchvision.__version__)\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "\n",
    "transforms.RandomRotation(0.7)\n",
    "transforms.RandomRotation([0.9, 0.2])\n",
    "\n",
    "t = transforms.RandomRotation(10)\n",
    "angle = t.get_params(t.degrees)\n",
    "\n",
    "print(angle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why PyTorch?\n",
    "\n",
    "*All the quotes will come from the PyTorch About Page http://pytorch.org/about/ from which I'll plagiarize shamelessly.  After all, who better to tout the virtues of PyTorch than the creators?*\n",
    "\n",
    "\n",
    "### What is PyTorch?\n",
    "\n",
    "According to the PyTorch about page, \"PyTorch is a python package that provides two high-level features:\n",
    "\n",
    "- Tensor computation (like numpy) with strong GPU acceleration\n",
    "- Deep Neural Networks built on a tape-based autograd system\"\n",
    "\n",
    "### Why is it getting so popular?\n",
    "\n",
    "#### It's quite fast\n",
    "\n",
    "\"PyTorch has minimal framework overhead. We integrate acceleration libraries such as Intel MKL and NVIDIA (CuDNN, NCCL) to maximize speed. At the core, it’s CPU and GPU Tensor and Neural Network backends (TH, THC, THNN, THCUNN) are written as independent libraries with a C99 API.\n",
    "They are mature and have been tested for years.\n",
    "\n",
    "Hence, PyTorch is quite fast – whether you run small or large neural networks.\"\n",
    "\n",
    "#### Imperative programming experience\n",
    "\n",
    "\"PyTorch is designed to be intuitive, linear in thought and easy to use. When you execute a line of code, it gets executed. There isn’t an asynchronous view of the world. When you drop into a debugger, or receive error messages and stack traces, understanding them is straight-forward. The stack-trace points to exactly where your code was defined. We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.\"\n",
    "\n",
    "\"PyTorch is not a Python binding into a monolothic C++ framework. It is built to be deeply integrated into Python. You can use it naturally like you would use numpy / scipy / scikit-learn etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as Cython and Numba. Our goal is to not reinvent the wheel where appropriate.\"\n",
    "\n",
    "#### Takes advantage of GPUs easily\n",
    "\n",
    "\"PyTorch provides Tensors that can live either on the CPU or the GPU, and accelerate compute by a huge amount.\n",
    "\n",
    "We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, math operations, linear algebra, reductions. And they are fast!\"\n",
    "\n",
    "\n",
    "#### Dynamic Graphs!!!\n",
    "\n",
    "\"Most frameworks such as TensorFlow, Theano, Caffe and CNTK have a static view of the world. One has to build a neural network, and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch.\n",
    "\n",
    "With PyTorch, we use a technique called Reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as autograd, autograd, Chainer, etc.\n",
    "\n",
    "While this technique is not unique to PyTorch, it’s one of the fastest implementations of it to date. You get the best of speed and flexibility for your crazy research.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with PyTorch Basics\n",
    "\n",
    "Enough of the sales pitch!  Let's start to understand the PyTorch basics.\n",
    "\n",
    "The basic unit of PyTorch is a tensor (basically a multi-dimensional array like a np.ndarray).\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/2000/1*_D5ZvufDS38WkhK9rK32hQ.jpeg)\n",
    "\n",
    "(image borrowed from https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32 )\n",
    "\n",
    "We can create PyTorch tensors directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1\n",
      " 2\n",
      " 3\n",
      " 4\n",
      " 5\n",
      " 6\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      "\n",
      " 1  2  3\n",
      " 4  5  6\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      "(0 ,.,.) = \n",
      "  1  2\n",
      "  3  4\n",
      "\n",
      "(1 ,.,.) = \n",
      "  5  6\n",
      "  7  8\n",
      "[torch.FloatTensor of size 2x2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## You can create torch.Tensor objects by giving them data directly\n",
    "\n",
    "#  1D vector\n",
    "vector_input = [1., 2., 3., 4., 5., 6.]\n",
    "vector = torch.Tensor(vector_input)\n",
    "\n",
    "# Matrix\n",
    "matrix_input = [[1., 2., 3.], [4., 5., 6]]\n",
    "matrix = torch.Tensor(matrix_input)\n",
    "\n",
    "# Create a 3D tensor of size 2x2x2.\n",
    "tensor_input = [[[1., 2.], [3., 4.]],\n",
    "          [[5., 6.], [7., 8.]]]\n",
    "tensor3d = torch.Tensor(tensor_input)\n",
    "\n",
    "\n",
    "print(vector)\n",
    "print(matrix)\n",
    "print(tensor3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They can be created without any initialization or initialized with random data from uniform (rand()) or normal (randn()) distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0000e+00  1.0842e-19  6.3095e+27  1.0845e-19  1.8217e-44\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "[torch.FloatTensor of size 2x5]\n",
      "\n",
      "\n",
      " 0.0000e+00  1.0842e-19  0.0000e+00  1.0842e-19  5.6052e-45\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  1.0842e-19\n",
      " 6.3059e+27 -1.5849e+29  2.8026e-45  1.0842e-19  4.9077e+27\n",
      "[torch.FloatTensor of size 3x5]\n",
      "\n",
      "\n",
      " 0.5759  0.0052  0.5583\n",
      " 0.6048  0.9838  0.5592\n",
      " 0.9206  0.8251  0.5032\n",
      " 0.5607  0.0485  0.4050\n",
      " 0.6590  0.7941  0.6106\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.0325  0.7753  0.2360  0.6659  0.7960\n",
      " 0.1888  0.4185  0.9106  0.8155  0.1502\n",
      " 0.6387  0.9303  0.7255  0.1813  0.5066\n",
      " 0.9799  0.9844  0.2526  0.0286  0.1560\n",
      " 0.8586  0.2915  0.5509  0.5185  0.5027\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n",
      "\n",
      "-1.7841 -0.1001 -0.6045\n",
      " 0.1409  0.6862  0.8469\n",
      " 0.8223  2.1229 -0.2956\n",
      " 0.6558 -1.1188 -0.2326\n",
      " 1.2631  0.2665 -0.0208\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      "-0.0194 -2.0925  0.9395 -0.0195  1.3913\n",
      " 1.9729  0.5524 -1.0353 -0.0404 -0.4854\n",
      "-0.3671 -0.1941 -0.6049 -1.7765  0.2992\n",
      " 0.1015 -1.2305 -1.1176  0.0383  1.0059\n",
      "-1.6558 -0.9154 -0.6085 -0.9777  0.8537\n",
      "[torch.FloatTensor of size 5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tensors with no initialization\n",
    "x_1 = torch.Tensor(2, 5)\n",
    "y_1 = torch.Tensor(3, 5)\n",
    "print(x_1)\n",
    "print(y_1)\n",
    "\n",
    "# Tensors initialized from uniform\n",
    "x_2 = torch.rand(5, 3)\n",
    "y_2 = torch.rand(5, 5)\n",
    "\n",
    "print(x_2)\n",
    "print(y_2)\n",
    "\n",
    "# Tensors initialized from normal\n",
    "x_3 = torch.randn(5, 3)\n",
    "y_3 = torch.randn(5, 5)\n",
    "\n",
    "print(x_3)\n",
    "print(y_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expected operations (arithmetic operations, addressing, etc) are all in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0000e+00  1.0842e-19  6.3095e+27  1.0845e-19  1.8217e-44\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "[torch.FloatTensor of size 2x5]\n",
      "\n",
      "\n",
      " 0.5759  0.0052  0.5583\n",
      " 0.6048  0.9838  0.5592\n",
      " 0.9206  0.8251  0.5032\n",
      " 0.5607  0.0485  0.4050\n",
      " 0.6590  0.7941  0.6106\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      "-1.7841 -0.1001 -0.6045\n",
      " 0.1409  0.6862  0.8469\n",
      " 0.8223  2.1229 -0.2956\n",
      " 0.6558 -1.1188 -0.2326\n",
      " 1.2631  0.2665 -0.0208\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      "-1.2081 -0.0949 -0.0462\n",
      " 0.7457  1.6700  1.4062\n",
      " 1.7429  2.9480  0.2075\n",
      " 1.2165 -1.0703  0.1724\n",
      " 1.9221  1.0606  0.5898\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      "-0.6045\n",
      " 0.8469\n",
      "-0.2956\n",
      "-0.2326\n",
      "-0.0208\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Expect (2,5)\n",
    "x_1.size()\n",
    "\n",
    "print(x_1)\n",
    "\n",
    "\n",
    "# Addition\n",
    "print(x_2)\n",
    "print(x_3)\n",
    "\n",
    "print(x_2+ x_3)\n",
    "\n",
    "# Addressing\n",
    "print(x_3[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to move between PyTorch and Numpy worlds with numpy() and torch.from_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0000e+00  1.0842e-19  6.3095e+27  1.0845e-19  1.8217e-44\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "[torch.FloatTensor of size 2x5]\n",
      "\n",
      "[[  0.00000000e+00   1.08420217e-19   6.30950545e+27   1.08446661e-19\n",
      "    1.82168800e-44]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "<class 'torch.FloatTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "# PyTorch --> Numpy\n",
    "print(x_1)\n",
    "print(x_1.numpy())\n",
    "\n",
    "print(type(x_1))\n",
    "print(type(x_1.numpy()))\n",
    "\n",
    "numpy_x_1 = x_1.numpy()\n",
    "pytorch_x_1 = torch.from_numpy(numpy_x_1)\n",
    "\n",
    "print(type(numpy_x_1))\n",
    "print(type(pytorch_x_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally PyTorch provides some convenience mechanisms for concatenating Tensors via torch.cat() and reshaping them with  .view() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.0000e+00  1.0842e-19  6.3095e+27  1.0845e-19  1.8217e-44\n",
      " 0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00  0.0000e+00\n",
      "[torch.FloatTensor of size 2x5]\n",
      "\n",
      "[[  0.00000000e+00   1.08420217e-19   6.30950545e+27   1.08446661e-19\n",
      "    1.82168800e-44]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "<class 'torch.FloatTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "## Concatenating\n",
    "\n",
    "# By default, it concatenates along the first axis (concatenates rows)\n",
    "x_1 = torch.randn(2, 5)\n",
    "y_1 = torch.randn(3, 5)\n",
    "z_1 = torch.cat([x_1, y_1])\n",
    "print(z_1)\n",
    "\n",
    "# Concatenate columns:\n",
    "x_2 = torch.randn(2, 3)\n",
    "y_2 = torch.randn(2, 5)\n",
    "# second arg specifies which axis to concat along\n",
    "z_2 = torch.cat([x_2, y_2], 1)\n",
    "print(z_2)\n",
    "\n",
    "## Reshaping\n",
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "print(x.view(2, 12))  # Reshape to 2 rows, 12 columns\n",
    "# Same as above.  If one of the dimensions is -1, its size can be inferred\n",
    "print(x.view(2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok -- in order to understand variables in PyTorch, let's take a break and learn about Artificial Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Variables and the Computational Graph\n",
    "\n",
    "Ok -- back to PyTorch.\n",
    "\n",
    "The other fundamental PyTorch construct besides Tensors are Variables.  Variables are very similar to tensors, but they also keep track of the graph (including their gradients for autodifferentiation).  They are defined in the autograd module of torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first variables gradient:  None\n",
      "first variables data:  \n",
      " 23.3000\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Let's create a variable by initializing it with a tensor\n",
    "first_tensor = torch.Tensor([23.3])\n",
    "\n",
    "first_variable = Variable(first_tensor, requires_grad=True)\n",
    "\n",
    "print(\"first variables gradient: \", first_variable.grad)\n",
    "print(\"first variables data: \", first_variable.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create some new variables. We can do so implicitly just by creating other variables with functional relationships to our variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.data:  \n",
      " 1.5409e+33\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "y.grad:  None\n",
      "z.data:  \n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "z.grad:  None\n",
      "x.grad: Variable containing:\n",
      " 0\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = first_variable\n",
    "y = (x ** x) * (x - 2) # y is a variable\n",
    "z = F.tanh(y) # z has a functional relationship to y, it's a variable\n",
    "z.backward()\n",
    "\n",
    "print(\"y.data: \", y.data)\n",
    "print(\"y.grad: \", y.grad)\n",
    "\n",
    "print(\"z.data: \", z.data)\n",
    "print(\"z.grad: \", z.grad)\n",
    "\n",
    "print(\"x.grad:\", x.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables come with a .backward() that allows them to do autodifferentiation via backwards propagation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a model with PyTorch\n",
    "\n",
    "Constructing a model with PyTorch is based on a design pattern with a fairly repeatable three step process:\n",
    "\n",
    "- Design your model (including relationships between your variables)\n",
    "    - Generally done by defining a subclass of torch.nn.Module\n",
    "- Construct your loss and optimizer\n",
    "- Train your model using your optimizer and forwards and backwards steps in your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import numpy as np\n",
    "np.random.seed(99)\n",
    "x1_data, y1_data, coef = make_regression(30,10, 10, bias=1, noise=2, coef=True)\n",
    "\n",
    "x1_data = [x1_data[i:i+1] for i in range(0, len(x1_data), 1)]\n",
    "y1_data = [y1_data[i:i+1] for i in range(0, len(y1_data), 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 726254.125\n",
      "1 124822.0\n",
      "2 54632.37890625\n",
      "3 28299.19921875\n",
      "4 15898.6708984375\n",
      "5 9425.998046875\n",
      "6 5830.21484375\n",
      "7 3738.6923828125\n",
      "8 2474.47607421875\n",
      "9 1683.9019775390625\n",
      "10 1174.31640625\n",
      "11 837.0162353515625\n",
      "12 608.6522827148438\n",
      "13 451.1288757324219\n",
      "14 340.83050537109375\n",
      "15 262.689208984375\n",
      "16 206.82933044433594\n",
      "17 166.6243896484375\n",
      "18 137.53953552246094\n",
      "19 116.42092895507812\n",
      "20 101.04288482666016\n",
      "21 89.822998046875\n",
      "22 81.62397003173828\n",
      "23 75.62706756591797\n",
      "24 71.23649597167969\n",
      "25 68.02033996582031\n",
      "26 65.66402435302734\n",
      "27 63.936065673828125\n",
      "28 62.66962432861328\n",
      "29 61.74079132080078\n",
      "30 61.05937957763672\n",
      "31 60.55976867675781\n",
      "32 60.1931037902832\n",
      "33 59.92445373535156\n",
      "34 59.7275505065918\n",
      "35 59.5826301574707\n",
      "36 59.47638702392578\n",
      "37 59.39829635620117\n",
      "38 59.341041564941406\n",
      "39 59.29940414428711\n",
      "40 59.26884078979492\n",
      "41 59.24623107910156\n",
      "42 59.229835510253906\n",
      "43 59.21748733520508\n",
      "44 59.208683013916016\n",
      "45 59.20254898071289\n",
      "46 59.197391510009766\n",
      "47 59.19389724731445\n",
      "48 59.19124984741211\n",
      "49 59.18930435180664\n",
      "50 59.1878662109375\n",
      "51 59.186649322509766\n",
      "52 59.18614959716797\n",
      "53 59.1855583190918\n",
      "54 59.185325622558594\n",
      "55 59.185020446777344\n",
      "56 59.18451690673828\n",
      "57 59.18470001220703\n",
      "58 59.18440628051758\n",
      "59 59.18421936035156\n",
      "60 59.184173583984375\n",
      "61 59.184513092041016\n",
      "62 59.18410110473633\n",
      "63 59.184043884277344\n",
      "64 59.18428421020508\n",
      "65 59.18429183959961\n",
      "66 59.18418884277344\n",
      "67 59.184200286865234\n",
      "68 59.18415832519531\n",
      "69 59.184078216552734\n",
      "70 59.18434143066406\n",
      "71 59.18395233154297\n",
      "72 59.184078216552734\n",
      "73 59.18425369262695\n",
      "74 59.18419647216797\n",
      "75 59.18400192260742\n",
      "76 59.18406295776367\n",
      "77 59.18445587158203\n",
      "78 59.183876037597656\n",
      "79 59.18396759033203\n",
      "80 59.184043884277344\n",
      "81 59.18427658081055\n",
      "82 59.1843376159668\n",
      "83 59.18402099609375\n",
      "84 59.18409729003906\n",
      "85 59.18416976928711\n",
      "86 59.184112548828125\n",
      "87 59.18427276611328\n",
      "88 59.184295654296875\n",
      "89 59.184444427490234\n",
      "90 59.18429183959961\n",
      "91 59.184329986572266\n",
      "92 59.184513092041016\n",
      "93 59.18413162231445\n",
      "94 59.18425369262695\n",
      "95 59.18431854248047\n",
      "96 59.18419647216797\n",
      "97 59.184288024902344\n",
      "98 59.184173583984375\n",
      "99 59.184226989746094\n",
      "100 59.18417739868164\n",
      "101 59.18421936035156\n",
      "102 59.18415832519531\n",
      "103 59.1842041015625\n",
      "104 59.18415451049805\n",
      "105 59.1842041015625\n",
      "106 59.18415832519531\n",
      "107 59.184207916259766\n",
      "108 59.184173583984375\n",
      "109 59.184207916259766\n",
      "110 59.18415451049805\n",
      "111 59.184200286865234\n",
      "112 59.18414306640625\n",
      "113 59.184226989746094\n",
      "114 59.184146881103516\n",
      "115 59.184226989746094\n",
      "116 59.18418502807617\n",
      "117 59.18434524536133\n",
      "118 59.18442153930664\n",
      "119 59.18431854248047\n",
      "120 59.184322357177734\n",
      "121 59.184295654296875\n",
      "122 59.18415069580078\n",
      "123 59.18414306640625\n",
      "124 59.18423843383789\n",
      "125 59.184165954589844\n",
      "126 59.18419647216797\n",
      "127 59.18421173095703\n",
      "128 59.184226989746094\n",
      "129 59.18421936035156\n",
      "130 59.18425750732422\n",
      "131 59.184226989746094\n",
      "132 59.184242248535156\n",
      "133 59.18424987792969\n",
      "134 59.184234619140625\n",
      "135 59.18421936035156\n",
      "136 59.18425750732422\n",
      "137 59.184226989746094\n",
      "138 59.184242248535156\n",
      "139 59.18424987792969\n",
      "140 59.184234619140625\n",
      "141 59.18421936035156\n",
      "142 59.18425750732422\n",
      "143 59.184226989746094\n",
      "144 59.184242248535156\n",
      "145 59.18424987792969\n",
      "146 59.184234619140625\n",
      "147 59.18421936035156\n",
      "148 59.18425750732422\n",
      "149 59.184226989746094\n",
      "150 59.184242248535156\n",
      "151 59.18424987792969\n",
      "152 59.184234619140625\n",
      "153 59.18421936035156\n",
      "154 59.18425750732422\n",
      "155 59.184226989746094\n",
      "156 59.184242248535156\n",
      "157 59.18424987792969\n",
      "158 59.184234619140625\n",
      "159 59.18421936035156\n",
      "160 59.18425750732422\n",
      "161 59.184226989746094\n",
      "162 59.184242248535156\n",
      "163 59.18424987792969\n",
      "164 59.184234619140625\n",
      "165 59.18421936035156\n",
      "166 59.18425750732422\n",
      "167 59.184226989746094\n",
      "168 59.184242248535156\n",
      "169 59.18424987792969\n",
      "170 59.184234619140625\n",
      "171 59.18421936035156\n",
      "172 59.18425750732422\n",
      "173 59.184226989746094\n",
      "174 59.184242248535156\n",
      "175 59.18424987792969\n",
      "176 59.184234619140625\n",
      "177 59.18421936035156\n",
      "178 59.18425750732422\n",
      "179 59.184226989746094\n",
      "180 59.184242248535156\n",
      "181 59.18424987792969\n",
      "182 59.184234619140625\n",
      "183 59.18421936035156\n",
      "184 59.18425750732422\n",
      "185 59.184226989746094\n",
      "186 59.184242248535156\n",
      "187 59.18424987792969\n",
      "188 59.184234619140625\n",
      "189 59.18421936035156\n",
      "190 59.18425750732422\n",
      "191 59.184226989746094\n",
      "192 59.184242248535156\n",
      "193 59.18424987792969\n",
      "194 59.184234619140625\n",
      "195 59.18421936035156\n",
      "196 59.18425750732422\n",
      "197 59.184226989746094\n",
      "198 59.184242248535156\n",
      "199 59.18424987792969\n",
      "200 59.184234619140625\n",
      "201 59.18421936035156\n",
      "202 59.18425750732422\n",
      "203 59.184226989746094\n",
      "204 59.184242248535156\n",
      "205 59.18424987792969\n",
      "206 59.184234619140625\n",
      "207 59.18421936035156\n",
      "208 59.18425750732422\n",
      "209 59.184226989746094\n",
      "210 59.184242248535156\n",
      "211 59.18424987792969\n",
      "212 59.184234619140625\n",
      "213 59.18421936035156\n",
      "214 59.18425750732422\n",
      "215 59.184226989746094\n",
      "216 59.184242248535156\n",
      "217 59.18424987792969\n",
      "218 59.184234619140625\n",
      "219 59.18421936035156\n",
      "220 59.18425750732422\n",
      "221 59.184226989746094\n",
      "222 59.184242248535156\n",
      "223 59.18424987792969\n",
      "224 59.184234619140625\n",
      "225 59.18421936035156\n",
      "226 59.18425750732422\n",
      "227 59.184226989746094\n",
      "228 59.184242248535156\n",
      "229 59.18424987792969\n",
      "230 59.184234619140625\n",
      "231 59.18421936035156\n",
      "232 59.18425750732422\n",
      "233 59.184226989746094\n",
      "234 59.184242248535156\n",
      "235 59.18424987792969\n",
      "236 59.184234619140625\n",
      "237 59.18421936035156\n",
      "238 59.18425750732422\n",
      "239 59.184226989746094\n",
      "240 59.184242248535156\n",
      "241 59.18424987792969\n",
      "242 59.184234619140625\n",
      "243 59.18421936035156\n",
      "244 59.18425750732422\n",
      "245 59.184226989746094\n",
      "246 59.184242248535156\n",
      "247 59.18424987792969\n",
      "248 59.184234619140625\n",
      "249 59.18421936035156\n",
      "250 59.18425750732422\n",
      "251 59.184226989746094\n",
      "252 59.184242248535156\n",
      "253 59.18424987792969\n",
      "254 59.184234619140625\n",
      "255 59.18421936035156\n",
      "256 59.18425750732422\n",
      "257 59.184226989746094\n",
      "258 59.184242248535156\n",
      "259 59.18424987792969\n",
      "260 59.184234619140625\n",
      "261 59.18421936035156\n",
      "262 59.18425750732422\n",
      "263 59.184226989746094\n",
      "264 59.184242248535156\n",
      "265 59.18424987792969\n",
      "266 59.184234619140625\n",
      "267 59.18421936035156\n",
      "268 59.18425750732422\n",
      "269 59.184226989746094\n",
      "270 59.184242248535156\n",
      "271 59.18424987792969\n",
      "272 59.184234619140625\n",
      "273 59.18421936035156\n",
      "274 59.18425750732422\n",
      "275 59.184226989746094\n",
      "276 59.184242248535156\n",
      "277 59.18424987792969\n",
      "278 59.184234619140625\n",
      "279 59.18421936035156\n",
      "280 59.18425750732422\n",
      "281 59.184226989746094\n",
      "282 59.184242248535156\n",
      "283 59.18424987792969\n",
      "284 59.184234619140625\n",
      "285 59.18421936035156\n",
      "286 59.18425750732422\n",
      "287 59.184226989746094\n",
      "288 59.184242248535156\n",
      "289 59.18424987792969\n",
      "290 59.184234619140625\n",
      "291 59.18421936035156\n",
      "292 59.18425750732422\n",
      "293 59.184226989746094\n",
      "294 59.184242248535156\n",
      "295 59.18424987792969\n",
      "296 59.184234619140625\n",
      "297 59.18421936035156\n",
      "298 59.18425750732422\n",
      "299 59.184226989746094\n",
      "300 59.184242248535156\n",
      "301 59.18424987792969\n",
      "302 59.184234619140625\n",
      "303 59.18421936035156\n",
      "304 59.18425750732422\n",
      "305 59.184226989746094\n",
      "306 59.184242248535156\n",
      "307 59.18424987792969\n",
      "308 59.184234619140625\n",
      "309 59.18421936035156\n",
      "310 59.18425750732422\n",
      "311 59.184226989746094\n",
      "312 59.184242248535156\n",
      "313 59.18424987792969\n",
      "314 59.184234619140625\n",
      "315 59.18421936035156\n",
      "316 59.18425750732422\n",
      "317 59.184226989746094\n",
      "318 59.184242248535156\n",
      "319 59.18424987792969\n",
      "320 59.184234619140625\n",
      "321 59.18421936035156\n",
      "322 59.18425750732422\n",
      "323 59.184226989746094\n",
      "324 59.184242248535156\n",
      "325 59.18424987792969\n",
      "326 59.184234619140625\n",
      "327 59.18421936035156\n",
      "328 59.18425750732422\n",
      "329 59.184226989746094\n",
      "330 59.184242248535156\n",
      "331 59.18424987792969\n",
      "332 59.184234619140625\n",
      "333 59.18421936035156\n",
      "334 59.18425750732422\n",
      "335 59.184226989746094\n",
      "336 59.184242248535156\n",
      "337 59.18424987792969\n",
      "338 59.184234619140625\n",
      "339 59.18421936035156\n",
      "340 59.18425750732422\n",
      "341 59.184226989746094\n",
      "342 59.184242248535156\n",
      "343 59.18424987792969\n",
      "344 59.184234619140625\n",
      "345 59.18421936035156\n",
      "346 59.18425750732422\n",
      "347 59.184226989746094\n",
      "348 59.184242248535156\n",
      "349 59.18424987792969\n",
      "350 59.184234619140625\n",
      "351 59.18421936035156\n",
      "352 59.18425750732422\n",
      "353 59.184226989746094\n",
      "354 59.184242248535156\n",
      "355 59.18424987792969\n",
      "356 59.184234619140625\n",
      "357 59.18421936035156\n",
      "358 59.18425750732422\n",
      "359 59.184226989746094\n",
      "360 59.184242248535156\n",
      "361 59.18424987792969\n",
      "362 59.184234619140625\n",
      "363 59.18421936035156\n",
      "364 59.18425750732422\n",
      "365 59.184226989746094\n",
      "366 59.184242248535156\n",
      "367 59.18424987792969\n",
      "368 59.184234619140625\n",
      "369 59.18421936035156\n",
      "370 59.18425750732422\n",
      "371 59.184226989746094\n",
      "372 59.184242248535156\n",
      "373 59.18424987792969\n",
      "374 59.184234619140625\n",
      "375 59.18421936035156\n",
      "376 59.18425750732422\n",
      "377 59.184226989746094\n",
      "378 59.184242248535156\n",
      "379 59.18424987792969\n",
      "380 59.184234619140625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381 59.18421936035156\n",
      "382 59.18425750732422\n",
      "383 59.184226989746094\n",
      "384 59.184242248535156\n",
      "385 59.18424987792969\n",
      "386 59.184234619140625\n",
      "387 59.18421936035156\n",
      "388 59.18425750732422\n",
      "389 59.184226989746094\n",
      "390 59.184242248535156\n",
      "391 59.18424987792969\n",
      "392 59.184234619140625\n",
      "393 59.18421936035156\n",
      "394 59.18425750732422\n",
      "395 59.184226989746094\n",
      "396 59.184242248535156\n",
      "397 59.18424987792969\n",
      "398 59.184234619140625\n",
      "399 59.18421936035156\n",
      "400 59.18425750732422\n",
      "401 59.184226989746094\n",
      "402 59.184242248535156\n",
      "403 59.18424987792969\n",
      "404 59.184234619140625\n",
      "405 59.18421936035156\n",
      "406 59.18425750732422\n",
      "407 59.184226989746094\n",
      "408 59.184242248535156\n",
      "409 59.18424987792969\n",
      "410 59.184234619140625\n",
      "411 59.18421936035156\n",
      "412 59.18425750732422\n",
      "413 59.184226989746094\n",
      "414 59.184242248535156\n",
      "415 59.18424987792969\n",
      "416 59.184234619140625\n",
      "417 59.18421936035156\n",
      "418 59.18425750732422\n",
      "419 59.184226989746094\n",
      "420 59.184242248535156\n",
      "421 59.18424987792969\n",
      "422 59.184234619140625\n",
      "423 59.18421936035156\n",
      "424 59.18425750732422\n",
      "425 59.184226989746094\n",
      "426 59.184242248535156\n",
      "427 59.18424987792969\n",
      "428 59.184234619140625\n",
      "429 59.18421936035156\n",
      "430 59.18425750732422\n",
      "431 59.184226989746094\n",
      "432 59.184242248535156\n",
      "433 59.18424987792969\n",
      "434 59.184234619140625\n",
      "435 59.18421936035156\n",
      "436 59.18425750732422\n",
      "437 59.184226989746094\n",
      "438 59.184242248535156\n",
      "439 59.18424987792969\n",
      "440 59.184234619140625\n",
      "441 59.18421936035156\n",
      "442 59.18425750732422\n",
      "443 59.184226989746094\n",
      "444 59.184242248535156\n",
      "445 59.18424987792969\n",
      "446 59.184234619140625\n",
      "447 59.18421936035156\n",
      "448 59.18425750732422\n",
      "449 59.184226989746094\n",
      "450 59.184242248535156\n",
      "451 59.18424987792969\n",
      "452 59.184234619140625\n",
      "453 59.18421936035156\n",
      "454 59.18425750732422\n",
      "455 59.184226989746094\n",
      "456 59.184242248535156\n",
      "457 59.18424987792969\n",
      "458 59.184234619140625\n",
      "459 59.18421936035156\n",
      "460 59.18425750732422\n",
      "461 59.184226989746094\n",
      "462 59.184242248535156\n",
      "463 59.18424987792969\n",
      "464 59.184234619140625\n",
      "465 59.18421936035156\n",
      "466 59.18425750732422\n",
      "467 59.184226989746094\n",
      "468 59.184242248535156\n",
      "469 59.18424987792969\n",
      "470 59.184234619140625\n",
      "471 59.18421936035156\n",
      "472 59.18425750732422\n",
      "473 59.184226989746094\n",
      "474 59.184242248535156\n",
      "475 59.18424987792969\n",
      "476 59.184234619140625\n",
      "477 59.18421936035156\n",
      "478 59.18425750732422\n",
      "479 59.184226989746094\n",
      "480 59.184242248535156\n",
      "481 59.18424987792969\n",
      "482 59.184234619140625\n",
      "483 59.18421936035156\n",
      "484 59.18425750732422\n",
      "485 59.184226989746094\n",
      "486 59.184242248535156\n",
      "487 59.18424987792969\n",
      "488 59.184234619140625\n",
      "489 59.18421936035156\n",
      "490 59.18425750732422\n",
      "491 59.184226989746094\n",
      "492 59.184242248535156\n",
      "493 59.18424987792969\n",
      "494 59.184234619140625\n",
      "495 59.18421936035156\n",
      "496 59.18425750732422\n",
      "497 59.184226989746094\n",
      "498 59.184242248535156\n",
      "499 59.18424987792969\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-354a79266859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# After training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mytrain_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = Variable(torch.Tensor(x1_data))\n",
    "y_data = Variable(torch.Tensor(y1_data))\n",
    "\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.linear = torch.nn.Linear(10, 1)  # One in and one out\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return\n",
    "        a Variable of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "\n",
    "# our model\n",
    "model = Model()\n",
    "\n",
    "\n",
    "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "# in the SGD constructor will contain the learnable parameters of the two\n",
    "# nn.Linear modules which are members of the model.\n",
    "criterion = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    \n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# After training\n",
    "ytrain_pred = model(x_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "nteract": {
   "version": "0.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
